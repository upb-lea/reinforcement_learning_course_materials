{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce73dc40222d6a29c3890ed189a80070",
     "grade": false,
     "grade_id": "cell-c97b01e39f10fa7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 12) Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "553611909b0cb216eb5a2f51b66cba5f",
     "grade": false,
     "grade_id": "cell-e732f039d63130ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations! You survived the course until the last exercise. \n",
    "\n",
    "![](https://media3.giphy.com/media/fwi2qY9VmH33uukTXr/giphy.gif)\n",
    "\n",
    "Source: https://giphy.com/gifs/Rambo-rambo-fwi2qY9VmH33uukTXr\n",
    "\n",
    "After this exercise, no semi-informed human resources department will be able to reject you. Showtime!\n",
    "\n",
    "In this exercise we will have a look at policy gradients. The theory of policy gradients applies to function approximators that decide on which action to choose. The function approximators we met in the past were employed to estimate the (action) value function. Since their task was to judge the quality of the current situation they are often referred to as \"critics\". In contrary, we can also use a function approximator to directly choose an action; we call these \"actors\". Why should we do that if we made it work with nothing more than a critic? Because this will finally allow us to make use of contiuous action spaces! Eureka!\n",
    "\n",
    "In this exercise we will use a new `gym` environment `LunarLanderContinuous-v2`.\n",
    "To run this environment please make sure to have `Box2D` installed: `pip install Box2D`.\n",
    "\n",
    "![](https://images.squarespace-cdn.com/content/v1/59e0d6f0197aea1a0abc8016/1507938542206-41S6K9T97YETKEHP0PQF/ke17ZwdGBToddI8pDm48kMR1yAHb8bPoH1-OdajP2rZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyDg3tXaPHS4cFkn9Bnm-emI0BDr_E-XKAFKqWrx68ZVlLyhCgVi_FJvVMH7mHrc18/lunar_lander_success_example.gif?format=500w)\n",
    "\n",
    "Source: https://www.billyvreeland.com/portfolio/2017/1013/solving-openai-gym-nm4yz\n",
    "\n",
    "The main task is to land the LunarLander in the landing zone.\n",
    "An accident-free landing is defined by both legs coming into  ground contact with moderate velocity.\n",
    "We are dealing with a continuous state and action space as defined below.\n",
    "Please notice that the control functions for main and side engines contain a dead zone in which the engines are inactive.\n",
    "The reward is mainly defined depending on whether the landing procedure is successful (+100) or not (-100).\n",
    "Firing the main engine gives a small negative reward. \n",
    "The problem is solved if a return of at least 200 is reached. \n",
    "For more information see https://gym.openai.com/envs/LunarLanderContinuous-v2/.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54737e71b40f2441795630ebbd1adc39",
     "grade": false,
     "grade_id": "cell-388eaa3e36f18307",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\text{state}&=\n",
    "\\begin{bmatrix}\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "v_x\\\\\n",
    "v_y\\\\\n",
    "\\varphi\\\\\n",
    "20 \\, \\omega\\\\\n",
    "1 \\text{ if left leg has ground contact, else } 0\\\\\n",
    "1 \\text{ if right leg has ground contact, else } 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\text{action}&=\n",
    "\\begin{bmatrix}\n",
    "\\text{main engine: } [-1, 0] \\rightarrow \\text{off}, ]0, 1] \\rightarrow [50 \\, \\%, 100 \\, \\%] \\text{ of available power}\\\\\n",
    "\\text{side engines: } [-1, -0.5] \\rightarrow [50 \\, \\%, 100 \\, \\%] \\text{ of available right engine power}, [0.5, 1] \\rightarrow [50 \\, \\%, 100 \\, \\%] \\text{ of available left engine power}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de17a48c1961fe633d253fdf1ef761c7",
     "grade": false,
     "grade_id": "cell-cb8f30d3aea6982a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-talk')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Lambda, Input\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5b0554e8057e58f290eb92569abaee6",
     "grade": false,
     "grade_id": "cell-8986527608544bb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "state = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6679a192bc679b53dadee5141e24d8b8",
     "grade": false,
     "grade_id": "cell-a5be36d9e2de2982",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1) Monte Carlo Policy Gradient\n",
    "Write a REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2429eff3a6feb2b87df2d80902bf159c",
     "grade": false,
     "grade_id": "cell-5fef942298a07621",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Execute the follwoing cell to fit the featurizer using RBFSampler, like already learned in the last exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "309e928343d2b4c8d2c162f665df24ca",
     "grade": false,
     "grade_id": "cell-6a414b0a2359c70d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "state_array = []\n",
    "state = env.reset()\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state_array.append(state)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "state_array = np.array(state_array)\n",
    "\n",
    "featurizer = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.preprocessing.StandardScaler(),\n",
    "    sklearn.pipeline.FeatureUnion([\n",
    "    (\"rbf0\", RBFSampler(gamma=5.0, n_components = 1000)),\n",
    "    (\"rbf1\", RBFSampler(gamma=2.0, n_components = 1000)),\n",
    "    (\"rbf2\", RBFSampler(gamma=1.0, n_components = 1000)),\n",
    "    (\"rbf3\", RBFSampler(gamma=0.5, n_components = 1000)),\n",
    "    ]),\n",
    "    sklearn.preprocessing.StandardScaler()\n",
    ")\n",
    "\n",
    "_ = featurizer.fit(state_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70a8241116bfd3590aed2a0657a499de",
     "grade": false,
     "grade_id": "cell-3c04ca9fbfe07902",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the following cell to define the function approximators for the policy.\n",
    "As seen in Algo.12.1 we need to calculate $\\nabla_\\theta \\mathrm{ln}\\,\\pi(u_k | x_k, \\theta)$.\n",
    "$\\pi$ is herein defined as the normal distribution : \n",
    "\\begin{align}\n",
    "\\pi(u_k | x_k, \\theta) & = \\frac{\\mathrm{exp} \\left( {-\\frac{1}{2} (u_k - \\mu_\\theta(x_k))^\\mathrm{T} \\mathbf{\\Sigma}^{-1}_\\theta(x_k) (u_k - \\mu_\\theta(x_k))} \\right)}{\\sqrt{(2\\pi)^p \\mathrm{det}(\\mathbf{\\Sigma}_\\theta(x_k))}},\\\\\n",
    "\\text{with}\\hspace{1em} p & = \\mathrm{dim}(u_k).\n",
    "\\end{align}\n",
    "\n",
    "Extend `loglikelyhoodGaussian` such that it returns $\\mathrm{ln}\\,\\pi(u_k | x_k, \\theta)$! \n",
    "Use the numpy equivalent `TensorFlow` functions (e.g. `tf.linalg.inv()`).\n",
    "`TensorFlow` functions are differentiable and can therefore be  used to calculate $\\nabla_\\theta \\mathrm{ln}\\,\\pi(u_k | x_k, \\theta)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29b7e1cf77ce977b570bf925a66d6f5e",
     "grade": false,
     "grade_id": "cell-451a7f093f5186bb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "state = np.reshape(env.reset(), (1, -1))\n",
    "feature_state = featurizer.transform(state)\n",
    "input_dim = feature_state.shape[1]\n",
    "action_space_dim = len(env.action_space.sample())\n",
    "\n",
    "\n",
    "# define policy\n",
    "def create_policy():\n",
    "    input_layer = Input(shape=input_dim)\n",
    "    \n",
    "    hidden1 = Dense(400, activation='linear')(input_layer)\n",
    "    hidden1 = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden1)\n",
    "        \n",
    "    hidden2_mu = Dense(400, activation='linear')(hidden1)\n",
    "    hidden2_mu = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden2_mu)\n",
    "    \n",
    "    hidden2_sigma = Dense(400, activation='linear')(hidden1)\n",
    "    hidden2_sigma = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden2_sigma)\n",
    "    \n",
    "    mu_out = Dense(action_space_dim, activation='linear')(hidden2_mu)\n",
    "    mu_out = Lambda(lambda x: tf.reshape(tf.clip_by_value(x, -1, 1), (-1, 1)))(mu_out)\n",
    "    sigma_out = Dense(action_space_dim, activation='softplus')(hidden2_sigma)\n",
    "    sigma_out = Lambda(lambda x: tf.linalg.diag(tf.clip_by_value(tf.reshape((x @ np.array([[0.01, 0], [0, 0.1]])), [-1]), 1e-4, 1)))(sigma_out)\n",
    "    \n",
    "    policy = Model(inputs=input_layer, outputs=[mu_out, sigma_out])\n",
    "    \n",
    "    return policy\n",
    "\n",
    "policy = create_policy()\n",
    "theta = policy.get_weights()\n",
    "\n",
    "# Regularization; downscaling of the network parameters to prevent divergence\n",
    "for i in range(len(theta)):\n",
    "    theta[i] = theta[i] * 0.4\n",
    "\n",
    "policy.set_weights(theta)\n",
    "\n",
    "@tf.function\n",
    "def loglikelyhoodGaussian(x, _mu, _sigma):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4351fac76aa842ce1e5ef8df25547c05",
     "grade": false,
     "grade_id": "cell-ed46e45053d3ffe3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following template to write a REINFORCE algorithm.\n",
    "This time the Adam (adaptive moment estimation) optimizer is used, which is an enhanced SGD optimizer.\n",
    "For more informations see https://arxiv.org/abs/1412.6980."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9700e75325a05bbc3c21d6dd8ec7739",
     "grade": false,
     "grade_id": "cell-d5b2fb9302cbf856",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "alpha_policy = 1e-5\n",
    "gamma = 0.99\n",
    "nb_episodes = 5000\n",
    "\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "return_history = []\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=alpha_policy)\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    k = 0\n",
    "    accumulated_rewards = 0\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs_log = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = state.reshape(1, -1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        while True:\n",
    "            # env.render()        \n",
    "            k += 1\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a81d073ccd5be9c56a22b8a17c57d734",
     "grade": false,
     "grade_id": "cell-10ea9f6de0129a23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the learning curve of the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a207f7b7ff15a6c1ed3a5aa0c51fdc0",
     "grade": false,
     "grade_id": "cell-9c960043e350900a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d225de00a6083f4b5a56535bcd8e5adc",
     "grade": false,
     "grade_id": "cell-88ab1f013663dfea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Execution\n",
    "\n",
    "Use `deterministic` to choose between deterministic execution (applying $\\mu$ directly) or taking the stochastic action by sampling from the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afc5344536bb9051d20937dbc1350b0a",
     "grade": false,
     "grade_id": "cell-5eaa6be280fc63bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "deterministic = True\n",
    "\n",
    "for j in tqdm(range(10)):\n",
    "        \n",
    "    state = env.reset()\n",
    "    accumulated_rewards = 0\n",
    "    \n",
    "    while True:\n",
    "        env.render()        \n",
    "        \n",
    "        ### STEP)\n",
    "        feat_state = featurizer.transform(np.reshape(state, (1, -1)))\n",
    "        mu, sigma = policy(feat_state)\n",
    "        print(mu)\n",
    "        if deterministic:\n",
    "            action = np.reshape(mu.numpy(),(-1,))\n",
    "        else:\n",
    "            action = np.random.multivariate_normal(mean=np.squeeze(mu.numpy()), cov=sigma.numpy())\n",
    "            \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "        accumulated_rewards += reward \n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            if j % 1 == 0:\n",
    "                print(f\"Episode {j}, Length: {k}\")\n",
    "                print(f\"Return {accumulated_rewards}\")\n",
    "                print()\n",
    "            break            \n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cad0cf9391dc79a84c1321dbab828dc",
     "grade": false,
     "grade_id": "cell-2436617277fd8ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Actor-Critic with TD(0) Targets\n",
    "\n",
    "Write an actor-critic (AC) algorithm to land the lander in the landing zone :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a80a9ffc6034adb77fcc6e3c94e6744",
     "grade": false,
     "grade_id": "cell-2f6e82b9fd39ab28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell to create two function approximators. One to estimate the state values (critic) and one to decide on the actions to take (actor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fd8c433d3899785e48fee2503a45635",
     "grade": false,
     "grade_id": "cell-ee84b13c4cd3bb0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state = np.reshape(env.reset(), (1, -1))\n",
    "input_dim = len(featurizer.transform(state)[0])\n",
    "action_space_dim = len(env.action_space.sample())\n",
    "\n",
    "# define critic\n",
    "def create_critic():\n",
    "    critic = Sequential()\n",
    "    critic.add(Dense(400, activation='linear', input_dim=input_dim))\n",
    "    critic.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    critic.add(Dense(400, activation='linear'))\n",
    "    critic.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    critic.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    return critic\n",
    "\n",
    "critic = create_critic()\n",
    "w = critic.get_weights()\n",
    "for i in range(len(w)):\n",
    "    w[i] = w[i] * 0.2\n",
    "critic.set_weights(w)\n",
    "\n",
    "# define actor\n",
    "def create_actor():\n",
    "    input_layer = Input(shape=input_dim)\n",
    "    \n",
    "    hidden1 = Dense(400, activation='linear')(input_layer)\n",
    "    hidden1 = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden1)\n",
    "    \n",
    "    hidden2 = Dense(400, activation='linear')(hidden1)\n",
    "    hidden2 = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden2) \n",
    "    \n",
    "    mu = Dense(action_space_dim, activation='linear')(hidden2)\n",
    "    mu = Lambda(lambda x: tf.reshape(tf.clip_by_value(x, -1, 1), (-1, 1)))(mu)\n",
    "    \n",
    "    sigma = Dense(action_space_dim, activation='softplus')(hidden2)\n",
    "    sigma = Lambda(lambda x: tf.linalg.diag(tf.clip_by_value(tf.reshape(x @ np.array([[0.01,   0], [   0, 0.1]]), [-1]), 1e-4, 1)))(sigma)\n",
    "    \n",
    "    actor = Model(inputs=input_layer, outputs=[mu, sigma])\n",
    "    \n",
    "    return actor\n",
    "\n",
    "actor = create_actor()\n",
    "theta = actor.get_weights()\n",
    "for i in range(len(theta)):\n",
    "    theta[i] = theta[i] * 0.4\n",
    "actor.set_weights(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2870b909e2b4d8655b80981450f21de",
     "grade": false,
     "grade_id": "cell-6a0128a78a488e3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following template to write an AC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0dcff16d8fc1eef0115e5936759589e",
     "grade": false,
     "grade_id": "cell-fa43dbbddfe825eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "\n",
    "alpha_critic = 1e-4\n",
    "alpha_actor = 1e-5\n",
    "gamma = 0.99\n",
    "nb_episodes = 2000\n",
    "\n",
    "\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    k = 0\n",
    "    accumulated_rewards = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    I = 1\n",
    "    \n",
    "    while True:\n",
    "        # env.render()        \n",
    "        k += 1\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape: \n",
    "            # persistent means the tape is not deleted after the first gradient has been computed\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if done:\n",
    "            if j % 250 == 0:\n",
    "                plt.plot(return_history, label='Return')\n",
    "                plt.plot(pd.Series(return_history, name='reward_history').rolling(10).mean(), label='MA')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('return')\n",
    "                plt.grid(True)\n",
    "                _=plt.legend()\n",
    "                plt.show()\n",
    "            return_history.append(accumulated_rewards)\n",
    "            break            \n",
    "        \n",
    "    env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de58e54250349b25aee825f2be2bd758",
     "grade": false,
     "grade_id": "cell-cff391e6a4178b47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot the learning curve of the training process!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a12b7fbee5ed0c3989df8b0082dc1cb6",
     "grade": false,
     "grade_id": "cell-ed5ed20c7c3d4932",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc4685e0410b0645d60d9e2e8bf80a1e",
     "grade": false,
     "grade_id": "cell-41b8edc65ba40233",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Execution\n",
    "\n",
    "Use `deterministic` to choose between deterministic execution (apply $\\mu$) directly or take the stochastic action by sampling from the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a44a59bc2ec8cf73daa34a14e011d9a",
     "grade": false,
     "grade_id": "cell-d7232e344bf16559",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "deterministic = True\n",
    "\n",
    "for j in tqdm(range(10)):\n",
    "        \n",
    "    state = env.reset()\n",
    "    accumulated_rewards = 0\n",
    "    \n",
    "    while True:\n",
    "        env.render()        \n",
    "        \n",
    "        ### STEP)\n",
    "        feat_state = featurizer.transform(np.reshape(state, (1, -1)))\n",
    "        mu, sigma = actor(feat_state)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = np.reshape(mu.numpy(), (-1,))\n",
    "        else:\n",
    "            action = np.random.multivariate_normal(mean=np.squeeze(mu.numpy()), cov=sigma.numpy())\n",
    "            \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "        accumulated_rewards += reward \n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            if j % 1 == 0:\n",
    "                print(f\"Episode {j}, Length: {k}\")\n",
    "                print(f\"Return {accumulated_rewards}\")\n",
    "                print()\n",
    "            break            \n",
    "        \n",
    "    env.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
