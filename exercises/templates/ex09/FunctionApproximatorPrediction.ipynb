{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b11a64192b629f82aa14c22468589c2",
     "grade": false,
     "grade_id": "cell-af7de1c2b8e61d18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 9) Function Approximators in Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "008e265414b1358eee38994adc143839",
     "grade": false,
     "grade_id": "cell-fbd6485cf4881324",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Up to now, we have used tabular methods to store data. These methods are rather simple concerning implementation and use, but they lack efficiency in several fields:\n",
    "\n",
    "- Tabular methods need a lot of storage capacity. Saving one number per possible state (or state-action combination) is very expensive for systems with a large discrete problem space. It even gets impossible when looking at continuous problem spaces.\n",
    "- Tabular methods are unable to generalize. Every update to the table only contains information about one specific state, which means that we only learn about states we have seen and not about the states that are \"near\" them. We can decide if we want to solve this issue by extending the training time or by lowering our expectations of the outcome.\n",
    "\n",
    "or alternatively, we make use of function approximators!\n",
    "\n",
    "For our investigations, we will have a look at the MountainCar environment from OpenAI's `gym`.\n",
    "\n",
    "This system has a continuous two-dimensional state space and a discrete one-dimensional action space.\n",
    "\n",
    "The MountainCar can be compared to the pendulum concerning that a successful policy must be able to perform a swing-up movement. The car has limited ability to accelerate uphill, it has to build up velocity by accelerating downhill. \n",
    "In contrast to the pendulum scenario, the MountainCar terminates upon reaching the goal on the mountaintop to the right. Every timestep will be rewarded with a reward of $r_{k+1}=-1$, such that it is most beneficial to end an episode as fast as possible. \n",
    "\n",
    "For this exercise we want to concentrate on the evaluation of an existing policy.\n",
    "\n",
    "![](https://marcinbogdanski.github.io/rl-sketchpad/Deep_Q_Network/assets/mountaincar.gif)\n",
    "\n",
    "(Source of GIF: https://marcinbogdanski.github.io/rl-sketchpad/Deep_Q_Network/1010_DQN_ClassicControl.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0302b26ecd5db2d92b4911832d3c2a0f",
     "grade": false,
     "grade_id": "cell-33ec88c4b59b3a7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please make sure to have `TensorFlow` installed:\n",
    "\n",
    "`pip install tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b348c21be0cfb027e1e9472fb53319f4",
     "grade": false,
     "grade_id": "cell-70e813bdcedc921c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-talk')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d2bd355a4d2863b796687bfc9227072",
     "grade": false,
     "grade_id": "cell-c5ca2799454a3b44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3b1893825e387a9fe9a76880b6ed0ae",
     "grade": false,
     "grade_id": "cell-a5a477ab9ba8a554",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    #env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8bcc812259afa66e6c33b7b4589cb20",
     "grade": false,
     "grade_id": "cell-0f213185bfcd49f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e14fa33701459b9ada2f1c4c93189cca",
     "grade": false,
     "grade_id": "cell-5683963ffdb84e94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell contains a simple policy for the swing-up of the MountainCar. We want to predict the value function with the use of a linear function approximator of the form:\n",
    "\n",
    "$\\hat{v}(\\mathbf{x_k})=\\mathbf{w}^\\text{T} \\tilde{\\mathbf{x}}_k$.\n",
    "\n",
    "Herein, the weight vector is denoted by $\\mathbf{w}$. The feature vector $\\tilde{\\mathbf{x}}_k$ is derived from the state vector $\\mathbf{x}_k$:\n",
    "\n",
    "$\\tilde{\\mathbf{x}}_k = f (\\mathbf{x}_k)$\n",
    "\n",
    "The state vector $\\mathbf{x}_k$ consists of the (vertical) position and the velocity.\n",
    "\n",
    "Write a Semi-Gradient TD(0) prediction algorithm that learns the weights of this linear value function approximator.\n",
    "Make use of a `feature` function, that accepts the state vector as an input and returns a feature vector that is derived from the state. The feature vector should be equal to zero ($\\tilde{\\mathbf{x}}_T = \\mathbf{0}$) if the finish line has been passed (this happens if the position is greater than $0.5$). Can you find a feature definition that enables a good value estimation?\n",
    "\n",
    "Hint:\n",
    "As it seems, the chances of successfully reaching the finish line rise when the car's energy increases. You may want to look at the [MountainCar sourcecode](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) to acquire expert knowledge for your feature definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b92fd5560d12ff0b11d4247175817e5",
     "grade": false,
     "grade_id": "cell-477b3082c50e0c32",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    # 0: left, 1: idle, 2: right\n",
    "    \n",
    "    pos, vel = state  \n",
    "    \n",
    "    # fixed policy, do not change\n",
    "    action = 2*int(vel > 0)\n",
    "        \n",
    "    return action\n",
    "\n",
    "def feature(state):\n",
    "    pos, vel = state\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return feature_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a119790647b77c8fad70f43369ae8c19",
     "grade": false,
     "grade_id": "cell-feb551d792c23d8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Semi-Gradient TD(0)\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "nb_episodes = 500\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "state = env.reset()\n",
    "feat_state = feature(state)\n",
    "w = np.zeros([len(feat_state)])\n",
    "visited_states = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38e714d594a0ece683b1ad59395c6331",
     "grade": false,
     "grade_id": "cell-2c70f952f9004c1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluate the result parameter vector by investigating the performance of the value function approximator on the whole state space, preferrably in a plot. Which parts of the estimation seem accurate, which do not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b225a994c19fe1e366cf5c85c878f88",
     "grade": false,
     "grade_id": "cell-f73c98b6fdf77664",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23e0b2c3e2c001dca820918a4cfd2cf9",
     "grade": false,
     "grade_id": "cell-4c0041ae3e474368",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14b8196a4f01f3353738d57299dc7b2d",
     "grade": false,
     "grade_id": "cell-3d2cc84be4954f81",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3b6307fed7b05ea1dacbfac40dee731",
     "grade": false,
     "grade_id": "cell-ea260243c55d68a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Recursive Least Squares TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cd7393d9475f9d6b55620cee6cf1839",
     "grade": false,
     "grade_id": "cell-bf761f1c6b1ed339",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the previous task we computed the parameters of the value function iteratively, by using old parameters and new observations to calculate new parameters. This works, but we can also be more data efficient. Recursive Least Squares Temporal Difference Learning (RLS-TD) allows to determine new parameters on the basis of new AND old observations, such that the parameters we receive are optimally fitted while taking past experiences into account. This method does not use a step size but only a forgetting factor $\\lambda \\in [0,1]$ which defines the impact of past experiences. \n",
    "\n",
    "Write an RLS-TD algorithm to solve the prediction problem. Check the stability of your code for the forgetting factors $\\lambda \\in \\{0.9, 0.99, 1\\}$. As this algorithm contains a lot of matrix multiplication, pay attention to vectors being represented within the correct dimensions. One could e.g. use the `row_vector.reshape(-1, 1)` command to turn a row vector into a column vector, or use the postix `.T` to transpose an array (note, this has no effect on one-dimensional arrays).\n",
    "\n",
    "The feature definition from task (1) can be reused here as long as $\\tilde{\\mathbf{x}}_T = \\mathbf{0}$ holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bc2b1694dd830fce899f08d417e54be",
     "grade": false,
     "grade_id": "cell-8850c630ca8dc45a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def feature(state):\n",
    "    pos, vel = state\n",
    "    win = int(pos > 0.5)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c454a44357d0d9007a4d86b5c1e13f65",
     "grade": false,
     "grade_id": "cell-d4d1b5df6fd0085c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "_lambda = 1 # we call it like that because \"lambda\" is a built-in command/ reserved syntax in python\n",
    "nb_episodes = 500\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "feat_state = feature(state)\n",
    "feat_dims = len(feat_state)\n",
    "\n",
    "P = np.eye(feat_dims)\n",
    "w = np.zeros((feat_dims, 1))  # column vector\n",
    "ident_mat = np.eye(feat_dims)  # identity matrix\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30ae462169756e2785329a1db541e54c",
     "grade": false,
     "grade_id": "cell-cb91c01d6d7933f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To simplify the comparison with your results from task (1), visualize the results from this task as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41680e88187329a17d0d9ce1e20885cf",
     "grade": false,
     "grade_id": "cell-cbe51a745b60cc78",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5088535936472622f2370fd761b27a5c",
     "grade": false,
     "grade_id": "cell-d2e06bbadb67860b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63f440df0a854a6c943e770886d0816f",
     "grade": false,
     "grade_id": "cell-801d6015e510d0c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Nonlinear Function Approximation with Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "770f724c3934e6aad374716bfd25c685",
     "grade": false,
     "grade_id": "cell-3b11571951447780",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Maybe we can achieve a more precise value estimation if we use a nonlinear function approximator. The dynamics of the environment are nonlinear, so it would be reasonable to expect a better result. We will use an artificial neural network as our function approximator without feature engineering. \n",
    "\n",
    "Neural networks tend to learn more reliably if the input variables are normalized. In this case, we will use minmax normalization. Write the function `normalize` that normalizes the MounatinCar state:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{state}&\\in\n",
    "\\begin{bmatrix}\n",
    "[-1.2, 0.6] \\\\\n",
    "[-0.07, 0.07]\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\text{normalize(state)}&\\in\n",
    "\\begin{bmatrix}\n",
    "[-1, 1] \\\\\n",
    "[-1, 1]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "`Tensorflow` is somewhat complicated to understand if you are new to it and efficient usage of it in reinforcement learning looks different from the usage in \"traditional\" supervised learning. That is why we prepared the code for the network training this time. You only need to write a proper `normalize` function here, but of course feel free to explore the learning algorithm (e.g. experiment with different ANN topologies) as a preparation for the upcoming exercises. A more detailed explanation about the usage of `Tensorflow` in this task will be presented in the corresponding tutorial video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c1f2158fd57987ac1ac810811c24ada",
     "grade": false,
     "grade_id": "cell-7b3dfb2ff2ab90d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def normalize(state):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return normed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35e64241b546cddebe7ef242f8c23aa2",
     "grade": false,
     "grade_id": "cell-89f39019a556ab31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "gamma = 0.9\n",
    "nb_episodes = 1000\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# define ANN topology\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "opt = SGD(learning_rate=alpha)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "errors = []\n",
    "visited_states = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    state = env.reset()\n",
    "    norm_state = normalize(state)\n",
    "    \n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # calculate the target (the value we want to estimate)\n",
    "        norm_next_state = normalize(next_state)\n",
    "        next_value = model(norm_next_state)\n",
    "        if not done:\n",
    "            target = reward + gamma * next_value\n",
    "        else:\n",
    "            target = np.array([[reward]])\n",
    "            \n",
    "        # calculate the loss (the prediction error)\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(norm_state)\n",
    "            loss = mse(target, prediction)\n",
    "            \n",
    "        # calculate and apply the gradients dMSE/dw\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        norm_state = norm_next_state\n",
    "        state = next_state\n",
    "        \n",
    "        errors.append(loss.numpy())\n",
    "        visited_states.append(state)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    #env.close()\n",
    "visited_states = np.vstack(visited_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2da9c8230b195d223f4a1d013c0d3452",
     "grade": false,
     "grade_id": "cell-0d47c78554097f0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is how the `relu` activation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faba66cf78cc07983d76b6f975001503",
     "grade": false,
     "grade_id": "cell-69a4a10c2e29c61c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)\n",
    "y = np.maximum(0, x)\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(x, y)\n",
    "plt.grid(True)\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-0.1, 1])\n",
    "plt.xlabel(r\"$x$\")\n",
    "_ = plt.ylabel(r\"$\\mathrm{relu}(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2158139c645fc671fdbbd63bb9cdc072",
     "grade": false,
     "grade_id": "cell-e6cb807f6067db3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Try to  visualize the results from this task as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b5710dcc0f6bab3fdf34aa42b4176a",
     "grade": false,
     "grade_id": "cell-a626b27e6cb55171",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "pos_vec = np.linspace(-1.2, 0.6, resolution)\n",
    "vel_vec = np.linspace(-0.07, 0.07, resolution)\n",
    "pos_mat, vel_mat = np.meshgrid(pos_vec, vel_vec)  # shapes: (100, 100), (100, 100)\n",
    "\n",
    "value_mat = model(normalize(\n",
    "                    np.column_stack([pos_mat.ravel(), vel_mat.ravel()])))\\\n",
    "                 .numpy().reshape(pos_mat.shape)\n",
    "\n",
    "# Plot\n",
    "plt.pcolormesh(pos_mat, vel_mat, value_mat)\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"velocity\")\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"V\")\n",
    "\n",
    "_ = plt.scatter(visited_states[:, 0], visited_states[:, 1], color=\"red\", s=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5802310686fc46e36d5b126efa5004dd",
     "grade": false,
     "grade_id": "cell-c52bf5721f292caf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "visited_values = model(normalize(visited_states)).numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(pos_mat, vel_mat, value_mat, cmap=\"viridis\")\n",
    "ax.scatter(visited_states[:, 0], visited_states[:, 1], np.squeeze(visited_values), color=\"red\")\n",
    "ax.set_xlabel('\\n\\nposition')\n",
    "ax.set_ylabel('\\n\\nvelocity')\n",
    "ax.set_zlabel('V')\n",
    "ax.view_init(30, -135)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
