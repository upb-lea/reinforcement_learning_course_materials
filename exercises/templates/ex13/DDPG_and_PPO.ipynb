{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e82265",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f90a383bbe8ef014db0f587cd6e18436",
     "grade": false,
     "grade_id": "cell-0ce69fe260e748d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercice 13) Deep Deterministic Policy Gradients and Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1767e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6260a348b7f48ae60460661e7ce0e0d5",
     "grade": false,
     "grade_id": "cell-5be3dd74ab09bc50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise we will investigate two state-of-the-art algorithms: deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO).\n",
    "\n",
    "We will examine their performance on [Goddard's rocket problem](https://github.com/osannolik/gym-goddard).\n",
    "This environment comes prepackaged in this notebook's folder, so it can be just imported.\n",
    "\n",
    "```\n",
    "First formulated by R. H. Goddard around 1910, this is a classical problem within dynamic optimization and optimal control. The task is simply to find the optimal thrust profile for a vertically ascending rocket in order for it to reach the maximum possible altitude, given that its mass decreases as the fuel is spent and that it is subject to varying drag and gravity.\n",
    "```\n",
    "\n",
    "The gym's observation space is the rocket's vertical position, velocity and mass, which are normalized for you to be within $[0, 1]$.\n",
    "\n",
    "The rocket engine is assumed to be throttled such that the thrust can be continuously controlled between 0 to some maximum limit, which translates to an action space $\\mathcal U \\in [0, 1]$.\n",
    "\n",
    "During the episode the reward is set to zero.\n",
    "If the episode is terminating, the maximum hight reached by the rocket during the experiment is used as reward.\n",
    "If the rocket has not depleted its tank after $300$ steps a penalty of $-1$ is applied and the episode terminates.\n",
    "\n",
    "![](rocket_spacex.jpg)\n",
    "(Photo by <a href=\"https://unsplash.com/@spacex?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">SpaceX</a> on <a href=\"https://unsplash.com/s/photos/rocket?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>)\n",
    "  \n",
    "\n",
    "In order for the full notebook to run through, you will also need the stable-baslines3 package.\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e497a9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b367e25d3afb72b88667dda4b6996395",
     "grade": false,
     "grade_id": "cell-f7ba0aeae957d480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from rocket_env import GoddardEnv\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from copy import deepcopy\n",
    "\n",
    "OPTIMAL_CONTROL = 0.0122079818367078\n",
    "RANDOM_AVG = 0.00995\n",
    "RANDOM_BEST = 0.01148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996669b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd741a120aa2f2478666d183ca1542a9",
     "grade": false,
     "grade_id": "cell-d77d2440cb4fa969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09281232",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ed224434a77e67fea152998a6a0644",
     "grade": false,
     "grade_id": "cell-027671c6914671f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Deep Deterministic Policy Gradient (DDPG) was first introduced [2015](https://arxiv.org/abs/1509.02971).\n",
    "It uses four neural networks to interact on the continuous state and action space.\n",
    "The first two we already know from the last exercise introduced Actor-Critic method: a critic to estimate the q-value function and an actor which deterministicly - completely trained - should give us the best action for a specific state with respect to the q-value function.\n",
    "Additionally, DDPG provides two target networks which are (in the beginning) copies of the actor and critic. These target networks are updated time-delayed in a low-pass filter manner.\n",
    "This enhances stability during the learning process because the networks used to calculate the target are not directly updated on the fly.\n",
    "For more information see for example [here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html).\n",
    "\n",
    "During the learning process the critic is updated based on minimizing (!) the following loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal L(w, {\\mathcal D}) = \\underset{(x,u,r,x',d) \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\n",
    "    \\Bigg(  \\left(r + \\gamma \\hat{q}_{}(x',\\pi(x', \\theta^-), w^-) \\right) - \\hat{q}(x,u, w) \\Bigg)^2\n",
    "    \\right].\n",
    "\\end{equation}   \n",
    "\n",
    "Here, the target is calculated using the target networks. $w$ and $w^-$ are the parameters of the critic and critic-target networks $\\hat{q}$, respectively and $\\theta^-$ define the parameters of the actor-target network.\n",
    "\n",
    "The policy/actor network is updated based on the idea to maximize (!) the expected return\n",
    "\n",
    "\\begin{align}\n",
    "    \\max_{\\theta} \\underset{x \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\\hat{q}(x, \\pi_{}(x, \\theta), w) \\right],\n",
    "\\end{align}  \n",
    "\n",
    "where $\\theta$ are the parameters of the policy network $\\pi$.\n",
    "\n",
    "The updates are performed off-policy by sampling from an experience replay buffer ${\\mathcal D}$.\n",
    "Since we are dealing with an deterministic actor, exploration during training is achieved by adding noise to the sampled actions.\n",
    "\n",
    "### Task: Implement the DDPG with PyTorch\n",
    "Write a DDPG algorithm using the Algo. 13.1 in lecture slides 13!\n",
    "A lot of the functions and classes are already given because we switch our framework here from Tensorflow to [PyTorch](https://pytorch.org/).\n",
    "There are a lot of tutorials on the PyTorch website to get familiar with how you can perform the update steps which will be most of your work in the follwoing cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6510a53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d5fef16377bb23d62de4e916a66f74d",
     "grade": false,
     "grade_id": "cell-947a0cd7668e21df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the following cell to make use of the defined multi layer perceptron and the plot function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2794f30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35f55ca3a0b374f1280e299441c255b0",
     "grade": false,
     "grade_id": "cell-8303cf3b2bb624b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    \"\"\"\n",
    "    Defines a multi layer perceptron using pytorch layers and activation funtions\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def plot_reward_trends(logs):\n",
    "    repeats = len(logs['envs'])\n",
    "    ncols = 2\n",
    "    nrows = int(np.ceil(repeats / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*8, nrows*4), sharex=True, sharey=True)\n",
    "    for ax, _env, _model in zip(axes.flatten(), logs['envs'], logs['models']):\n",
    "        ax.plot(_env.get_episode_rewards(), label='rewards', color='green')\n",
    "\n",
    "        ax.set_ylabel('Reward')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylim(0, 0.015)\n",
    "        ax.axhline(OPTIMAL_CONTROL, ls='--', color='red', label='optimal')\n",
    "        ax.axhline(RANDOM_BEST, ls='--', color='orange', label='best rnd actions')\n",
    "        ax.axhline(RANDOM_AVG, ls='--', color='yellow', label='average rnd actions')\n",
    "        ax.legend(loc='lower left')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(_env.get_episode_lengths(), label='ep lengths')\n",
    "        ax2.legend()\n",
    "        ax2.set_ylabel('Steps')\n",
    "    fig.tight_layout()\n",
    "\n",
    "def test_agent(logs):\n",
    "    \"\"\"Deterministic test of the agent in the given env\"\"\"    \n",
    "    num_test_episodes = 400\n",
    "    for i, (env, agent) in enumerate(zip(logs['envs'], logs['models'])):\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "        state = env.reset()\n",
    "        for j in tqdm(range(num_test_episodes)):\n",
    "            episode_len += 1\n",
    "            action = agent.decide(state, deterministic=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            env.render()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                break\n",
    "\n",
    "        print(f'Reward during test ({i}): {episode_reward}')\n",
    "        env.close()\n",
    "    print('Optimal control: 0.0122')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe6650",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8dc7dbd9d832c57e7df6c61c76592e52",
     "grade": false,
     "grade_id": "cell-b4bd4f5ae4c71df8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the following cells to make use of the predefined actor & critic and the replay buffer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cc30a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0975c8715cdc5d5fb128304c4e784e45",
     "grade": false,
     "grade_id": "cell-93290e96cc879232",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=nn.ReLU, act_limit=1):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Sigmoid)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.act_limit * self.pi(state)\n",
    "\n",
    "    def act(self, state):\n",
    "        # HINT: toch.no_grad -> result is not needed for backpropagation\n",
    "        with torch.no_grad():\n",
    "            return self.act_limit * self.pi(state).numpy()\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # torch.cat concatenates action to state\n",
    "\n",
    "        q = self.q(torch.cat([state, action], dim=-1))\n",
    "        return torch.squeeze(q, -1)  # To ensure q has right shape.\n",
    "    \n",
    "class ReplayBuffer:\n",
    "\n",
    "        def __init__(self, obs_dim, action_dim, buffer_size):\n",
    "            \n",
    "            self.state_buf = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
    "            self.next_state_buf = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
    "            self.action_buf = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "            self.reward_buf = np.zeros(buffer_size, dtype=np.float32)\n",
    "            self.done_buf = np.zeros(buffer_size, dtype=np.float32)\n",
    "            self.ptr, self.size, self.max_size = 0, 0, buffer_size\n",
    "\n",
    "        def push(self, state, action, reward, next_state, done):\n",
    "            \n",
    "            self.state_buf[self.ptr] = state\n",
    "            self.next_state_buf[self.ptr] = next_state\n",
    "            self.action_buf[self.ptr] = action\n",
    "            self.reward_buf[self.ptr] = reward\n",
    "            self.done_buf[self.ptr] = done\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        def fetch(self, batch_size=32):\n",
    "            \n",
    "            idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "            \n",
    "            batch = dict(state=self.state_buf[idxs],\n",
    "                         next_state=self.next_state_buf[idxs],\n",
    "                         action=self.action_buf[idxs],\n",
    "                         reward=self.reward_buf[idxs],\n",
    "                         done=self.done_buf[idxs])\n",
    "            \n",
    "            return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1c963",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb863c769b97307b480cb8709aed025f",
     "grade": false,
     "grade_id": "cell-2639a410b1080568",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here, fill in the following code template to write a DDPG agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a8f8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfddba78cec9e6330f633aab870e6b69",
     "grade": false,
     "grade_id": "cell-fbb8e1d539cd0caf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DDPG_agent:\n",
    "    \"\"\"Reference:\n",
    "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, env, actor_hidden_size, actor_number_layers, critic_hidden_size, critic_number_layers,\n",
    "                 buffer_size, actor_lr, critic_lr, gamma, batch_size, learning_starts, tau):#, action_noise):\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_starts = learning_starts\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        \n",
    "\n",
    "        # define A&C and the replaybuffer        \n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.replay_buffer = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # defines target networks\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        \n",
    "        # Uses Adam optimizer (see to ex12 for more explanation)\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # define action noise space for clipping\n",
    "        self.action_space_high = env.action_space.high\n",
    "        self.action_space_low = env.action_space.low\n",
    "\n",
    "    def q_loss(self, data):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def pi_loss(self, data):\n",
    "        \"\"\"\n",
    "        Calulate the loss for a gradient ascent(!) step\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "    def deliberate(self, number_updates_per_step=1):\n",
    "        \"\"\"\n",
    "        Fetches number_updates_per_step-times from replay_buffer, computes targets and calculates losses to \n",
    "        update the q-function using gradient descent and the policy function using gradient ascent\n",
    "        In the end updates the target networks\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(number_updates_per_step):\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def decide(self, state, deterministic=False, noise_scale=0.1):\n",
    "        \"\"\"\n",
    "        Returns action as nd-array depending on the state, adds scaled noise \n",
    "        and clips the action depending on the action space\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21ea71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72b30954db1893927f97e1c249f41b2b",
     "grade": false,
     "grade_id": "cell-f1983e65eedd72ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cell to train your agent. Note that, in contrast to classic RL applications, in standard libraries you have often the possibility to update the agent after full episodes instead of stepwise.\n",
    "This could be helpful in our application due to the sparse reward signal of the rocket environment instead of after each environment step.\n",
    "To overcome this issue we introduce the variable `learning_starts` and run here first 500 steps in the environment to be sure that we have at least one full episode in the replay buffer to learn from.\n",
    "\n",
    "To compare episode-wise vs step-wise updating, both is executed 4 times in the following cell.\n",
    "The learning could take some minutes. If you want to reduce it you can do only 1 run for each update for example (reduce `repeats`) or run less steps at all (reduce `total_timesteps`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b24aea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf58dd5ab40f3ee313873c6e188f6bca",
     "grade": false,
     "grade_id": "cell-c7977ed401a0f23b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_logs_ddpg = {'episode': {'envs': [], 'models': []}, 'step_wise': {'envs': [], 'models': []}}\n",
    "repeats = 4\n",
    "\n",
    "total_timesteps = 20000\n",
    "\n",
    "\n",
    "for train_freq, logs_ddpg in train_logs_ddpg.items():\n",
    "\n",
    "    for rep in range(repeats):\n",
    "\n",
    "\n",
    "        env = Monitor(GoddardEnv())\n",
    "\n",
    "        myDDPG_agent = DDPG_agent(env=env, actor_hidden_size=8, actor_number_layers=1,\n",
    "                               critic_hidden_size=8, critic_number_layers=1, buffer_size=int(1e6),\n",
    "                               actor_lr=1e-4, critic_lr=1e-4, gamma=0.999, batch_size=256, learning_starts=500, tau=0.005)\n",
    "\n",
    "        number_updates_per_step = 0\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "        rewards = []\n",
    "        episode_len_vec = []\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        for j in tqdm(range(total_timesteps)):\n",
    "            episode_len += 1\n",
    "            number_updates_per_step += 1\n",
    "            action = myDDPG_agent.decide(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            myDDPG_agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            #env.render()\n",
    "            if train_freq == 'step_wise' and j > myDDPG_agent.learning_starts:\n",
    "                # Train every step using myDDPG_agent.batch_size batch\n",
    "                myDDPG_agent.deliberate()\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if train_freq == 'episode' and j > myDDPG_agent.learning_starts:\n",
    "                # Train after each episode number_updates_per_step times using myDDPG_agent.batch_size batch\n",
    "                    myDDPG_agent.deliberate(number_updates_per_step)\n",
    "                state = env.reset()\n",
    "                rewards.append(episode_reward)\n",
    "                episode_len_vec.append(episode_len)\n",
    "                episode_reward = 0\n",
    "                episode_len = 0\n",
    "                number_updates_per_step = 0\n",
    "\n",
    "        logs_ddpg['envs'] += [env]\n",
    "        logs_ddpg['models'] += [myDDPG_agent]\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab8e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "969f4c8ca2e1492e8bafc6f26bdefaf6",
     "grade": false,
     "grade_id": "cell-f63074dfa3920133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ddpg['episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36782e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec41f65d2e7dd68bed0d8ef236c96d5b",
     "grade": false,
     "grade_id": "cell-ddffaf6c11d49a3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ddpg['step_wise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba7d82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a98f456bc82cabf59e8411637b246652",
     "grade": false,
     "grade_id": "cell-04b7fa3a7ef613f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell to test your agent on the env using deterministic actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69920904",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea551dbc32006d8cd7368a3e0c7bead0",
     "grade": false,
     "grade_id": "cell-b85d8e3dd9ad383b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_agent(train_logs_ddpg['episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dc107",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b7b82ff4cf7044ed6ebf7dc8765ed42",
     "grade": false,
     "grade_id": "cell-94f13e77c62bc467",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_agent(train_logs_ddpg['step_wise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b739a95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ac2cb2f80deb2bf72dfb06db1971ba7",
     "grade": false,
     "grade_id": "cell-57ec48fad41194c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: StableBaselines3 usage\n",
    "\n",
    "Alternatively, you can use readily available Python packages such as [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) for employment of state-of-the-art algorithms.\n",
    "In what follows below, the DDPG algorithm as utilized by stable-baselines3 is showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50d901",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "830d4627bd82ac70aa11afefa47607e2",
     "grade": false,
     "grade_id": "cell-4bc64d70ae9709c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "total_timesteps = int(20e3)\n",
    "\n",
    "\n",
    "class Pbar(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pbar = tqdm(desc='Training', total=total_timesteps)\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"This event is triggered before updating the policy.\"\"\"\n",
    "        # self.pbar.update(self.)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.pbar.update()\n",
    "        return True\n",
    "    \n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[8], vf=[8])])\n",
    "\n",
    "repeats = 4\n",
    "logs = {'envs': [], 'models': []}\n",
    "for _ in range(repeats):\n",
    "    \n",
    "    env = Monitor(GoddardEnv())\n",
    "    \n",
    "    #n_actions = env.action_space.shape[-1]\n",
    "    #noise_var = 4\n",
    "    #noise_theta = 25  # stiffness of OU\n",
    "    #action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), theta=noise_theta * np.ones(n_actions),\n",
    "    #                                        sigma=noise_var * np.ones(n_actions))\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    \n",
    "    policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=dict(pi=[8] * 1, qf=[8] * 1))\n",
    "    model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, batch_size=256, device='cpu',\n",
    "             learning_rate=1e-4, gamma=0.999, learning_starts=500)\n",
    "    model.learn(total_timesteps=total_timesteps, callback=Pbar())\n",
    "    logs['envs'] += [env]\n",
    "    logs['models'] += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f467ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e54f4ad98a1a9a08b66c45796640c6b7",
     "grade": false,
     "grade_id": "cell-354440d67e2e0b95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48081d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6683a41cd6cd9b421105c1c4af506f94",
     "grade": false,
     "grade_id": "cell-f7ad3146465beb22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell to execute the last trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e586e64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e947e8b77505f0eabf09f9d64d32bf4",
     "grade": false,
     "grade_id": "cell-c25a8b2418b0a9a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "max_steps_per_episode = 500\n",
    "tst_logs = {'rewards': []}\n",
    "for ep in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = np.zeros(1, dtype=np.bool)\n",
    "    cum_rew = 0\n",
    "    k = 0\n",
    "    while not np.all(done) and k < max_steps_per_episode:\n",
    "        action, _ = model.predict(state, deterministic=True) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        cum_rew += reward\n",
    "        k += 1\n",
    "\n",
    "    tst_logs['rewards'].append(cum_rew)\n",
    "rewards = np.array(tst_logs['rewards']).ravel()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c1a283",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82f5a81602541bebb6daa6efff074c06",
     "grade": false,
     "grade_id": "cell-038ec2a3065ebfd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) PPO\n",
    "\n",
    "The [original paper from 2017](https://arxiv.org/abs/1707.06347) for the PPO came up with an idea to combine A3C (having multiple workers) and TRPO (using a trust region to improve the actor).\n",
    "The PPO algorithm achieves this by hard clipping gradients in order to ensure that new policies won't be too far away from old ones.\n",
    "\n",
    "In contrast to DDPG, PPO is an on-policy algorithm. In order to still apply mini-batch training, there is a so-called roll-out-buffer that is filled up with the current policy, on whose base a gradient ascent update would be done.\n",
    "\n",
    "There are two variants on the PPO algorithm, from which we will implement the clip variant.\n",
    "\n",
    "The actor (policy) update is computed according to\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_{k+1} &= \\arg \\max_{\\theta} \\underset{\\langle x,u \\rangle \\sim \\pi_{\\theta_k}}{{\\mathbb E}}\\left[\\mathcal L(X,U,\\theta_k, \\theta)\\right], \\\\\n",
    "\\mathcal L(x,u,\\theta_k,\\theta) &= \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_k}(u|x)}  A^{\\pi_{\\theta_k}}(x,u), \\;\\;\n",
    "\\text{clip}\\left(\\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_k}(u|x)}, 1 - \\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_k}}(x, u)\n",
    "\\right),\n",
    "\\end{align}\n",
    "where the expectation operator denotes the empirical average across the roll-out-buffer, $\\theta$ the possible next weights of the actor, $\\epsilon$ denotes the threshold within the next update is allowed, and $\\mathcal L$ the long-term return.\n",
    "\n",
    "On the opposite side, the critic's weights $\\omega_k$ are updated through the plain stochastic mini-batch gradient descent on the mean squared error $\\mathcal C$ between immediately seen rewards $r_{k+1}$ and corresponding estimated value $v_{w_k}(x_k)$ across the roll-out-buffer:\n",
    "\\begin{align}\n",
    "w_{k+1} &\\leftarrow \\omega_k - \\alpha \\nabla\\mathcal C(X,R_{k+1},\\omega_k), \\\\\n",
    "\\mathcal C(x,r_{k+1}, \\omega_k) &= \\left(v_{\\omega_k} (x_k)- r_{k+1}\\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae076f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61841e23f8a548788fb194f40f5c0545",
     "grade": false,
     "grade_id": "cell-37e0d6f3e79fb7f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task: Implement the PPO clip variant with PyTorch\n",
    "Fill in the below code to run the PPO-clip variant on the rocket environment.\n",
    "\n",
    "For simplicity, do not contemplate a vectorized environment for synchronous training (like in A2C or A3C).\n",
    "Moreover, calculate the generalized advantage estimate with $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf443e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c6b15a3cfe8a0b01f1f749190a3122d",
     "grade": false,
     "grade_id": "cell-bfaedbfa4a92491d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class StochActor(nn.Module):\n",
    "    \"\"\"This stochastic actor learns a Gaussian distribution. While the mean value \n",
    "    is learnt by a full-fledged MLP, the standard deviation is denoted by a\n",
    "    single trainable weight. With Pytorch's distribution package, probabilities \n",
    "    given a certain distribution and an action can be calculated.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # The standard deviation is just one trainable weight\n",
    "        self.log_std = torch.nn.Parameter(\n",
    "                        torch.as_tensor(\n",
    "                            -.5 * np.ones(action_dim, dtype=np.float32)))\n",
    "        # the mean value is estimated by a full MLP\n",
    "        self.mu_net = mlp(list(state_dim) + [8] + list(action_dim), nn.Tanh, nn.Sigmoid)\n",
    "        \n",
    "    def _distribution(self, state):\n",
    "        return torch.distributions.normal.Normal(\n",
    "                self.mu_net(state), torch.exp(self.log_std))\n",
    "    \n",
    "    def forward(self, state, action=None):\n",
    "        pi = self._distribution(state)\n",
    "        # if action is None, logp_a will be, too\n",
    "        if action is None:\n",
    "            logp_a = None\n",
    "        else:\n",
    "            logp_a = pi.log_prob(action).sum(axis=-1)\n",
    "        return pi, logp_a\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"Reference:\n",
    "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/ppo.py\n",
    "    \"\"\"\n",
    "    \n",
    "    class RolloutBuffer:\n",
    "        def __init__(self, size, action_dim, state_dim):\n",
    "            self.action_buf = np.zeros((size, action_dim[0]), dtype=np.float32)\n",
    "            self.state_buf = np.zeros((size, state_dim[0]), dtype=np.float32)\n",
    "            self.rew_buf = np.zeros(size+1, dtype=np.float32)\n",
    "            self.val_buf = np.zeros(size+1, dtype=np.float32)\n",
    "            self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "            self.i = 0\n",
    "            self.max_size = size\n",
    "            \n",
    "        def push(self, state, action, reward, value, logp):\n",
    "            \"\"\"Append a sample to the buffer\"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        def fetch(self, last_val=None, gamma=0.999):\n",
    "            \"\"\"Get all data from the rollout buffer.\n",
    "            Returns a dictionary for state, action, rewards-to-go, advantages, and logp values.\n",
    "            Entries might be of different length across multiple calls to this function as episodes\n",
    "            potentially have different lengths. Rewards-to-go and advantages need to be computed from \n",
    "            rewards, values, and gamma, as outlined in https://arxiv.org/abs/1506.02438 (or as done\n",
    "            in the spinning up implementation). Normalize the advantage batch by standard scaling.\"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def __init__(self, buffer_size, env):\n",
    "        self.buf = self.RolloutBuffer(buffer_size, action_dim=env.action_space.shape,\n",
    "                                 state_dim=env.observation_space.shape)\n",
    "        # use the below attributes for the actor-critic agent\n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.n_epochs = 8\n",
    "        self.clip_ratio = 0.1\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def v_loss(self, data):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def pi_loss(self, data):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def deliberate(self, last_value=None):\n",
    "        \"\"\"Fetch the rollout buffer for all samples.\n",
    "        Train the actor and critic for n_epochs by computing the pi and v\n",
    "        loss iteratively.\n",
    "        The rewards and values buffer needs a zero appended for terminal states,\n",
    "        or the critic's estimate appended in case of intermediate states \n",
    "        that result from timeout. This is denoted by last_value.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def decide(self, state, deterministic=False):\n",
    "        \"\"\"Receive a state as torch tensor and return a tuple of numpy ndarrays\n",
    "        in the form (action, value, log-probability-of-action-under-pi)\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088532df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66799b6240e326501d2cbb6cac2b4b59",
     "grade": false,
     "grade_id": "cell-4cfa3ab6af86faef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# main training loop\n",
    "\n",
    "n_episodes = 250\n",
    "max_steps_per_episode = 500\n",
    "train_logs_ppo_custom = {'envs': [], 'models': []}\n",
    "\n",
    "def train_ppo(repeats):\n",
    "    for rep in range(repeats):\n",
    "        env = Monitor(GoddardEnv())\n",
    "        agent = PPOAgent(max_steps_per_episode, env)\n",
    "\n",
    "        for ep in tqdm(range(n_episodes), desc='Training'):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            k = 0\n",
    "            while not done and k < max_steps_per_episode:\n",
    "                action, value, logp_a = agent.decide(state) \n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                # track experience\n",
    "                agent.buf.push(state, action, reward, value, logp_a)\n",
    "\n",
    "                state = next_state\n",
    "                k += 1\n",
    "\n",
    "            # if episode finished with done signal, the long term value should be 0\n",
    "            #  for this terminal state\n",
    "            if done:\n",
    "                last_value = None\n",
    "            else:\n",
    "                _, last_value, _ = self.decide(state)\n",
    "            agent.deliberate(last_value)\n",
    "\n",
    "        train_logs_ppo_custom['envs'] += [env]\n",
    "        train_logs_ppo_custom['models'] += [agent]\n",
    "        env.close()\n",
    "train_ppo(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9741a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aafaa7441e23545b17ddb5e9093626b9",
     "grade": false,
     "grade_id": "cell-ec0e800aac84b980",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ppo_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae173c63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "014a0598a117294cb426c82ad3920a0d",
     "grade": false,
     "grade_id": "cell-f3ba7a0d376cc7ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_agent(train_logs_ppo_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880e463",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6194e42b2c25903c93280ad7c2c6bf7e",
     "grade": false,
     "grade_id": "cell-94626f12d2902d40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# more samples\n",
    "train_ppo(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc6bc2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "983e1f9105d916e1aeccdf23d3c856a5",
     "grade": false,
     "grade_id": "cell-2f80ed5f342a854e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot average and standard deviation of 50 runs\n",
    "repeats = len(train_logs_ppo_custom['envs'])\n",
    "rewards_mat = np.asarray([_env.get_episode_rewards()[:250] for _env in train_logs_ppo_custom['envs']])\n",
    "rewards_average = np.median(rewards_mat, axis=0)\n",
    "rewards_std = rewards_mat.std(axis=0)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(rewards_average, color='green', label='median reward')\n",
    "ax.plot(rewards_mat.max(axis=0), color='cyan', label='best seen reward')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylim(0, 0.015)\n",
    "ax.axhline(OPTIMAL_CONTROL, ls='--', color='red', label='optimal')\n",
    "ax.axhline(RANDOM_BEST, ls='--', color='orange', label='best rnd actions')\n",
    "ax.axhline(RANDOM_AVG, ls='--', color='yellow', label='average rnd actions')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_title('Summary of 50 experiments')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467185ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c2ccfc2a59db210cea1f164df1b441c",
     "grade": false,
     "grade_id": "cell-fe61925f62283573",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Very strong performance is sometimes achieved even within 10 training episodes. Here, the 'optimal' performance is based on an analytical solution using a continuous-time model (planning) which is applied to an approximate discrete-time rocket simulation. Hence, there is a systematic deviation between the model of the analytical solution and the actual simulation behavior (i.e., it is actual non-optimal w.r.t. the simplified simulation).\n",
    "\n",
    "The low median reward curve shows that several repetitions of an algorithm are inevitable for a robust assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a9323",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "133c994b969234991e52ab4d0b78e2ae",
     "grade": false,
     "grade_id": "cell-cd9a6dc2f628c73d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: StableBaselines3 usage\n",
    "\n",
    "Alternatively you can use readily available Python packages such as [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) for employment of state-of-the-art algorithms.\n",
    "In what follows below, the PPO algorithm as utilized by stable-baselines3 is showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b1f02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e8cf4631b82ee2d214eb5b5f85c57e5",
     "grade": false,
     "grade_id": "cell-b91e000d33246cbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Pbar(BaseCallback):\n",
    "    \"\"\"This is a callback that helps monitoring the training progress\"\"\"\n",
    "    def __init__(self, total, n_rollout_steps):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_rollout_steps\n",
    "        self.pbar = tqdm(desc='Training', total=total)\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"This event is triggered before updating the policy.\"\"\"\n",
    "        self.pbar.update(self.n_steps)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    \n",
    "policy_kwargs = dict(activation_fn=torch.nn.Sigmoid, net_arch=[dict(pi=[8], vf=[8])])\n",
    "\n",
    "n_steps = 256  # an episode takes around ~250 steps\n",
    "total_timesteps = n_steps*250  # steps_per_episode * episodes\n",
    "\n",
    "repeats = 4  # repeat experiment x times to assess scatter through random init\n",
    "train_logs_ppo_sb3 = {'envs': [], 'models': []}\n",
    "for _ in range(repeats):\n",
    "    \n",
    "    env = Monitor(GoddardEnv())\n",
    "    model = PPO('MlpPolicy', env, n_steps=n_steps, n_epochs=8,  verbose=0, device='cpu', learning_rate=1e-3, batch_size=n_steps, policy_kwargs=policy_kwargs,\n",
    "               clip_range=0.1, clip_range_vf=0.1, gamma=0.999)\n",
    "    model = model.learn(total_timesteps=total_timesteps, callback=Pbar(total_timesteps, n_steps))\n",
    "    train_logs_ppo_sb3['envs'] += [env]\n",
    "    train_logs_ppo_sb3['models'] += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc4167",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d622f1dfc171c14b7d6ce3cf2ea659",
     "grade": false,
     "grade_id": "cell-531e062c849ee81b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ppo_sb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca40f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a8a4d779b61587dab6f6027f6408f52",
     "grade": false,
     "grade_id": "cell-78eedb7acc89df22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Obviously, the chosen architecture and hyper parameters do well often, but can also fail abruptly. \n",
    "What might be the reason for these failures?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
