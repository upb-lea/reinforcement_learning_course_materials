import numpy as np
import random


class RaceTrackEnv:
    """
    RaceTrackEnv object maintains and updates the race track 
    state. Interaction with the class is through
    the step() method (see OpenAI Gym interface)
    
    The class constructor is given a race course as a list of 
    strings. The constructor loads the course and initializes 
    the environment state.
    """
    
    lbl2int = {'o': 1, '-': 0, '+': 2, 'W': -1}
    
    def __init__(self, course):
        """
        Load race course, set any min or max limits in the 
        environment (e.g. max speed), and set initial state.
        Initial state is random position on start line with 
        velocity = (0, 0).
        
        Example:
            tiny_right_turn_course = 
                  ['WWWWWW',
                   'Woooo+',
                   'Woooo+',
                   'WooWWW',
                   'WooWWW',
                   'WooWWW',
                   'WooWWW',
                   'W--WWW',]
        
        Args:
            course: List of text strings used to construct
                race-track.
                    '-': start line
                    '+': finish line
                    'o': track
                    'W': wall
        
        Returns:
            self
        """
        self.MAX_VELOCITY = 5
        self.start_positions = []
        self.course = None
        self._load_course(course)
        self._random_start_position()
        self.velocity = np.array([0, 0], dtype=np.int8)
        self.bounds = (len(course), len(course[0]))


    def step(self, action):
        """
        Perform given action on the environment.
        The reward for every action and state is -1, except for when 
        reaching the finish line.
        
        Args:
            action: integer-encoded {0-8} or a 
                2-tuple (a_y, a_x) in {[-1, 0, 1], [-1, 0, 1]}.
        
        Returns:
            5-tuple: (new_state, reward, terminated, truncated, info)
                new_state: the new state resulting from taken action
                reward: The reward obtained from taken action
                terminated: Bool. Whether we are in a terminal state.
                truncated: Bool. Whether we have to reset, but not finished the race track.
                Info: Dict. Arbitrary information.
                
        """
        ### BEGIN SOLUTION
        if not isinstance(action, tuple):
            action = self.action_to_tuple(action)
        
        reward = -1.0
        terminated = False
        truncated = False
        if self.is_terminal_state():
            reward = 0
            terminated = True
        else:
            # update velocity
            self.velocity += np.array(action, dtype=np.int8)
            self.velocity = np.clip(self.velocity, -self.MAX_VELOCITY, self.MAX_VELOCITY)
            # project jump to new position virtually
            projected_path_y = np.clip(
                np.around(np.linspace(self.position[0], 
                                      self.position[0] + self.velocity[0],
                                      self.MAX_VELOCITY*2),
                          decimals=1).astype(int), 
                0, self.bounds[0]-1)
            projected_path_x = np.clip(
                np.around(np.linspace(self.position[1], 
                                      self.position[1] + self.velocity[1],
                                      self.MAX_VELOCITY*2),
                          decimals=1).astype(int),
                0, self.bounds[1]-1)
            projected_steps = list(self.course[projected_path_y, projected_path_x])
            # hits finish line?
            if self.lbl2int['+'] in projected_steps:                
                # has grass been hit before?
                if self.lbl2int['W'] in projected_steps:
                    s = self.reset()
                    if projected_steps.index(self.lbl2int['+']) < projected_steps.index(self.lbl2int['W']):
                        # finished and hit grass afterwards
                        terminated = True
                    else:
                        truncated = True
                else:
                    # clean finish
                    self.position += self.velocity
                    s = self.get_state()
                    terminated = True
            # hits grass?
            elif self.lbl2int['W'] in projected_steps:
                truncated = True
                s = self.get_state()
            else:
                self.position += self.velocity
                s = self.get_state()
        return s, reward, terminated, truncated, {}
        ### END SOLUTION

    def get_state(self):
        """Returns state as a 4-element tuple."""
        return tuple([*self.position.copy(), *self.velocity.copy()])
            
            
    def reset(self):
        """Set velocity to 0 in both directions and set the position to any
        of the possible start positions.
        Returns the resulting state."""
        ### BEGIN SOLUTION
        self._random_start_position()
        self.velocity = np.array([0, 0], dtype=np.int8)
        ### END SOLUTION
        return self.get_state()

    def _random_start_position(self):
        """Set agent to random position on start line."""
        ### BEGIN SOLUTION
        self.position = np.array(random.choice(self.start_positions),
                                dtype=np.int8)
        ### END SOLUTION
    
    def _load_course(self, course):
        """Load given course. The course is expected to be a list of strings.
        Each string represents a horizontal line of the track.
        See __init__ doc.
        The course is to be internally represented as numpy array."""
        ### BEGIN SOLUTION
        y_size, x_size = len(course), len(course[0])
        self.course = np.zeros((y_size, x_size), dtype=np.int8)
        
        for y in range(y_size):
            self.course[y, :] = np.array([self.lbl2int[i] for i in course[y]], dtype=np.int8)
        
        self.start_positions = [(i,j) for i,j in zip(*np.where(self.course == self.lbl2int['-']))]                
        ### END SOLUTION

    def _is_finish(self, pos):
        """Return True if given position is in finish line"""
        ### BEGIN SOLUTION
        return self.course[pos[0], pos[1]] == self.lbl2int['+']
        ### END SOLUTION
    
    def is_terminal_state(self):
        """Return True at episode terminal state"""
        return self._is_finish(self.position)
    
    def action_to_tuple(self, a):
        """Convert integer action to 2-tuple: (ay, ax)"""
        ### BEGIN SOLUTION
        ay = a // 3 - 1
        ax = a % 3 - 1
        
        return ay, ax
        ### END SOLUTION
    
    def tuple_to_action(self, a):
        """Convert 2-tuple to integer action: {0-8}.
        Since there are two axes that can go forward, backward or 
        idle, we have 3Â² actions"""
        ### BEGIN SOLUTION
        return int((a[0] + 1) * 3 + a[1] + 1)
        ### END SOLUTION
    
    def state_action(self, s, a):
        """Build a state-action tuple for indexing Q NumPy array."""
        if not isinstance(a, tuple):
            a = self.action_to_tuple(a)

        if len(s) == 4:
            s_y, s_x, s_vy, s_vx = s
        else:
            p, v = s       
            s_y, s_x = p[0], p[1]
            s_vy, s_vx = v[0], v[1]

        a_y, a_x = a[0]+1, a[1]+1
        return s_y, s_x, s_vy, s_vx, a_y, a_x
    
    def render():
        """Render the current position of the agent within the track text-based"""
        ### BEGIN SOLUTION
        pass
        ### END SOLUTION


class StochRaceTrackEnv(RaceTrackEnv):
    """
    Driver is not able to perceive the environment clearly anymore
    """
    def __init__(self, eps=0.2, **kwargs):
        self.eps = eps        
        super().__init__(**kwargs)
        # centrifugal acceleration map
        self.cam = np.zeros((2,) + self.course.shape, dtype=int)
        self.cam[0, :self.course.shape[0]//2, :] = -1
        self.cam[0, self.course.shape[0]//2:, :] = 1
        self.cam[1, :, :self.course.shape[1]//2] = -1
        self.cam[1, :, self.course.shape[1]//2:] = 1
     
    def step(self, action):
        if isinstance(action, int):
            action = self.action_to_tuple(action)
        
        prev_p, prev_v = tuple(self.position), tuple(self.velocity)
        
        
        state, reward, terminated, truncated, _ = super().step(action)
        
        # sample random reward if centrifugal acceleration applies
        if action[0] == self.cam[0, prev_p[0], prev_p[1]] and action[1] == self.cam[1, prev_p[0], prev_p[1]]:
            reward += np.random.randn()

        return state, reward, terminated, truncated, _
