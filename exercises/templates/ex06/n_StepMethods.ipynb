{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf21c49d1e5fc66a3747c806d62c4a01",
     "grade": false,
     "grade_id": "cell-c833698b7dad927d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 6: n-Step Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4174f12aa1ae9ad406012928cfe0632d",
     "grade": false,
     "grade_id": "cell-7cf627dacfec200a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise we will have a look at n-step methods. This class of reinforcement learning algorithms is an abstraction of the previously discussed Monte-Carlo and TD(0) methods and includes them as special cases. The environment we will be dealing with is a little more typical for control research: the inverted pendulum. \n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*TNo3x9zDi1lVOH_3ncG7Aw.gif)\n",
    "\n",
    "To implement this environment, we will make use of the gymnasium library. Please install the gymnasium library within your preferred Python environment using:\n",
    "\n",
    "```pip install gymnasium```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7453fe419c839ddb6ef1a21b8be23281",
     "grade": false,
     "grade_id": "cell-68ba542456c544d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50a139d58ebe2c067271409d4d4635bd",
     "grade": false,
     "grade_id": "cell-b9853bfaec1d8013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check if the installation and import work by executing the following cell. A window with an animation of the pendulum should open, display some random actions, and close automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77fcce7f101c43978af6d15c39794f6e",
     "grade": false,
     "grade_id": "cell-8e133fbe2615fd5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped # removes a built-in time limit of k_T = 200, we want to determine the time limit ourselves\n",
    "\n",
    "state, _ = env.reset()\n",
    "for _ in range(300):\n",
    "    env.render()\n",
    "    state, reward, terminated, _, _ = env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72fad190bc1716968cbce798322e1ce1",
     "grade": false,
     "grade_id": "cell-f29246bdc3e421c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The goal of this environment is to bring the pendulum into the upper neutral position, where the angle $\\theta = 0$ and the angular velocitiy $\\frac{\\text{d}}{\\text{d}t}\\theta=\\omega=0$. The reward function is already designed that way and does not need further specification. For further information about the environment you may refer to the code and documentation of Farama Foundation's `gymnasium`:\n",
    "\n",
    "[Documentation of the gymnasium pendulum](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
    "\n",
    "[Pendulum environment in the gymnasium Github repository](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/pendulum.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2a638f16d68f332fc0f3a82e032b73f",
     "grade": false,
     "grade_id": "cell-8570b84cc28ffc4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Discretization of Action and State Space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b2e9234766f6cd32f5786eb89c2e565",
     "grade": false,
     "grade_id": "cell-ee59383187979c94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Unlike the racetrack environment, the inverted pendulum comes with a continuous action and state space. Although it is possible to handle systems with these characteristics, we did not yet learn how to deal with them. For now, we only know how to implement agents for discrete action and state spaces. Accordingly, we will also try to represent the inverted pendulum within a discrete state / action space. For this, a discretization is necessary.\n",
    "\n",
    "The pendulum has three state variables relating to the momentary angular position $\\theta$:\n",
    "\\begin{align*}\n",
    "    x=\\begin{bmatrix}\n",
    "    \\text{cos}(\\theta)\\\\\n",
    "    \\text{sin}(\\theta)\\\\\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\theta\n",
    "    \\end{bmatrix}\n",
    "    \\in\n",
    "    \\begin{bmatrix}\n",
    "    [-1, 1]\\\\\n",
    "    [-1, 1]\\\\\n",
    "    [-8 \\, \\frac{1}{\\text{s}}, 8 \\, \\frac{1}{\\text{s}}]\n",
    "    \\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "and one input variable which relates to the torque applied at the axis of rotation:\n",
    "\n",
    "$u = T \\in [-2 \\, \\text{N}\\cdot\\text{m}, 2 \\, \\text{N}\\cdot\\text{m}]$\n",
    "\n",
    "After the discretization, we want the system to be defined on sets of non-negative natural numbers:\n",
    "\n",
    "\\begin{align*}\n",
    "    x_d =\n",
    "    \\text{discretize_state}(x)\n",
    "    \\in\n",
    "    \\begin{bmatrix}\n",
    "    \\{0,1,2,...,d_{\\theta}-1\\}\\\\\n",
    "    \\{0,1,2,...,d_{\\theta}-1\\}\\\\\n",
    "    \\{0,1,2,...,d_{\\omega}-1\\}\n",
    "    \\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "$\n",
    "u_d =\n",
    "\\text{discretize_action}(u)\n",
    "\\in\n",
    "\\{0,1,2,...,d_{T}-1\\}.\n",
    "$\n",
    "\n",
    "Since action is selected within the discrete action space, we need to transform it accordingly:\n",
    "\n",
    "$\n",
    "u=\n",
    "\\text{continualize_action}(u_d):\n",
    "\\{0,1,2,...,d_{T}-1\\} \\rightarrow [-2 \\, \\text{N}\\cdot\\text{m}, 2 \\, \\text{N}\\cdot\\text{m}]\n",
    ".\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d67be3dc735ad3e75b98d3a35cc4cce",
     "grade": false,
     "grade_id": "cell-28b6b992373b4a65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write the functions `discretize_state` and `continualize_action`, such that a discrete RL agent can be applied. (Please note that all I/O of `gymnasium` consists of numpy arrays.) Write the functions in such a way that the number of discretization intervals $d_\\theta, d_\\omega, d_T$ are parameters that can be changed for different tests. The discretization intervals should be uniformly distributed on their respective state space.\n",
    "\n",
    "A parametrization of $d_\\theta = d_\\omega = d_T = 15$ can be used to yield satisfactory results in this exercise.\n",
    "However, does it make a difference if the number of discretization intervals is odd or even? If yes, what should be preferred for the given environment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "786fa6ead07a16cc0540a14e9cbaf99a",
     "grade": false,
     "grade_id": "cell-cfd4a4f7a22ce35c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Solution 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b63df0573df481b3efcf661e77df3be",
     "grade": true,
     "grade_id": "cell-cf67ba4807c7ce8c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51d9cbff476230783271d2b3fdd1f8a1",
     "grade": false,
     "grade_id": "cell-af38d4d166803785",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d_T = 15\n",
    "d_theta = 15\n",
    "d_omega = 15\n",
    "\n",
    "def discretize_state(states):   \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "def continualize_action(disc_action):    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04823d55c0a413ce03bb36f6ccc0ed79",
     "grade": false,
     "grade_id": "cell-54296429c6a25f98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1a5e3ee8841433dd538973d993edce0",
     "grade": false,
     "grade_id": "cell-755b0b9277910870",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "state = env.reset()\n",
    "for _ in range(5):\n",
    "    disc_action = np.random.choice(range(9))\n",
    "    cont_action = continualize_action(disc_action)\n",
    "    print(\"discrete action: {}, continuous action: {}\".format(disc_action, cont_action))\n",
    "    \n",
    "    state, reward, terminated, _, _ = env.step(cont_action) # take a random action\n",
    "    disc_state = discretize_state(state)\n",
    "    print(\"discrete state: {}, continuous state: {}\".format(disc_state, state))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49a4d732185a7169c64e97cb97873700",
     "grade": true,
     "grade_id": "cell-7050729ab9b288bc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bfbde17bf71060103fa2eb7c9a140d7",
     "grade": true,
     "grade_id": "cell-191ecd76d4787fb5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70213d8b39435fe626032697aadb5f19",
     "grade": false,
     "grade_id": "cell-2575b2d2065717a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) n-Step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "378e6a1c5e0d6e2fcf84910c6ec437fc",
     "grade": false,
     "grade_id": "cell-71c349849a7bdad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write an on-policy n-step Sarsa control algorithm for the inverted pendulum from scratch. This time, no code template is given. \n",
    "\n",
    "Use the following parameters: $\\alpha=0.1, \\gamma=0.9, \\varepsilon=0.1, n=10$ with 500 time steps in 2000 episodes.\n",
    "\n",
    "![](nStepSARSA_Algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a42894ad9d9ad093e424f9aad26e362",
     "grade": true,
     "grade_id": "cell-877e2e0ac6a7510e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44c680e80677431ef1afebdc018b41a6",
     "grade": false,
     "grade_id": "cell-37c9e8a6c5268048",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.1  # epsilon greedy parameter\n",
    "n = 10  # steps between updates\n",
    "\n",
    "nb_episodes = 2000  # number of episodes\n",
    "nb_steps = 500  # length of episodes\n",
    "\n",
    "action_values = np.zeros([d_theta, d_theta, d_omega, d_T])\n",
    "# int is necessary for indexing\n",
    "pi = np.zeros([d_theta, d_theta, d_omega], dtype=int)\n",
    "\n",
    "# we can use this to figure out how well the learning worked\n",
    "cumulative_reward_history = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes), position=0, leave=True):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "pi_learned = np.copy(pi)  # save pi in cache under different name for later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a70529957ec84e9354815ebb1a42cf3b",
     "grade": true,
     "grade_id": "cell-fc8a6d2fe73f71bf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "310528718879e1d68cd72cc27c699a51",
     "grade": false,
     "grade_id": "cell-ddebe8848a817b91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Greedy Execution\n",
    "\n",
    "Test the learned policy by pure greedy execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b88b3a47d51d86a1faa7209413560e8",
     "grade": false,
     "grade_id": "cell-6ffa29bb63e9fc42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "nb_steps = 200\n",
    "\n",
    "state = env.reset() # initialize x_0\n",
    "disc_state = tuple(discretize_state(state)) # use tuple indexing\n",
    "disc_action = pi_learned[disc_state]\n",
    "\n",
    "for k in range(nb_steps):\n",
    "        \n",
    "    cont_action = continualize_action(disc_action)\n",
    "    env.render() # comment out for faster execution\n",
    "    state, reward, terminated, _, _ = env.step(cont_action)\n",
    "    disc_state = tuple(discretize_state(state))\n",
    "        \n",
    "    if terminated:\n",
    "        break\n",
    "        \n",
    "    disc_action = pi_learned[disc_state] # exploitative action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86c9128d1df9057e025ea71ef5543da5",
     "grade": false,
     "grade_id": "cell-2cb21bf8569c915a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Tree Backups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14ebb5cb96856ea5d8c501bc68ff9276",
     "grade": false,
     "grade_id": "cell-ef9ba6fd56f7dc65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Although n-step Sarsa is a very powerful algorithm, it still needs to be trained on-policy. This is not a problem in simulations, but it might be quite dangerous if used on physical systems. Therefore, we also need an off-policy solution. \n",
    "\n",
    "Use the policy learned in task (2) as a behavior policy when implementing n-step Sarsa with tree backups and compare their learning behavior. Be aware that execution may be time consuming.\n",
    "\n",
    "Use the following parameters: $\\alpha=0.1, \\gamma=0.9, \\varepsilon=0.1, n=5$ with 500 time steps in 10 000 episodes (might take some time).\n",
    "\n",
    "What can we say about the training process? What can we say about the resulting learned policy? Did the agent learn a good policy? Why? Why not?\n",
    "\n",
    "![](nStepTreeBackup_Algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db149ad47361a0ce49a94a2fe4643062",
     "grade": false,
     "grade_id": "cell-eb4adfe7b82e148f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.1 # 0.1 # epsilon greedy parameter\n",
    "n = 5 # steps between updates\n",
    "\n",
    "nb_episodes = 10000 # number of episodes\n",
    "nb_steps = 500 # length of episodes\n",
    "\n",
    "action_values = -999 * np.ones([d_theta, d_theta, d_omega, d_T])\n",
    "behavior_policy = np.copy(pi_learned) # pi_learned should be the learned policy from (2), make sure it is active\n",
    "pi = np.zeros([d_theta, d_theta, d_omega], dtype=int)\n",
    "\n",
    "cumulative_reward_history = [] # we can use this to figure out how well the learning worked\n",
    "\n",
    "for j in tqdm(range(nb_episodes), position=0, leave=True):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5dab58f9f34567257b836d0791e5912",
     "grade": false,
     "grade_id": "cell-0eb6391db0077e71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e267f5515c35f5c480985a9195fcf6e8",
     "grade": false,
     "grade_id": "cell-c1d56f58872a3dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Greedy Execution\n",
    "\n",
    "Test the learned policy by pure greedy execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d7cede4dc74b15895548b4b08d359f5",
     "grade": false,
     "grade_id": "cell-563faef7628f6cf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "nb_steps = 200\n",
    "\n",
    "\n",
    "state = env.reset() # initialize x_0\n",
    "disc_state = tuple(discretize_state(state)) # use tuple indexing\n",
    "disc_action = pi[disc_state]\n",
    "\n",
    "for k in range(nb_steps):\n",
    "        \n",
    "    cont_action = continualize_action(disc_action)\n",
    "    env.render() # comment out for faster execution\n",
    "    state, reward, terminated, _, _ = env.step(cont_action)\n",
    "    disc_state = tuple(discretize_state(state))\n",
    "        \n",
    "    if terminated:\n",
    "        break\n",
    "        \n",
    "    disc_action = pi[disc_state] # exploitative action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddd83a47c66d50530978965da1330397",
     "grade": false,
     "grade_id": "cell-1efb4bbc72ef2f47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4) Comprehensive: n-Step $Q(\\sigma)$ Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd328ce30dc49b6ed18c3b562e555dbe",
     "grade": false,
     "grade_id": "cell-c99f9c778095b841",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The $Q(\\sigma)$ algorithm allows for even more flexibility. Like in n-step Sarsa, we can again choose the number $n$ of past steps to consider for an update. Moreover, we can choose the parameter $\\sigma$ to change the weighting between sampled and expected state transitions. Accordingly, a properly tuned $Q(\\sigma)$ agent is very flexible and thus powerful.\n",
    "\n",
    "Unfortunately, with great power comes great responsibility. Namely, the responsibility to make a good decision on the parameters. What is a \"properly tuned\" agent? This is a very basic question of RL and now we want to investigate it by the means of a (small) hyperparameter optimization that utilizes simple grid search.\n",
    "\n",
    "Write an off-policy epsilon-greedy $Q(\\sigma)$ ($\\pi$ greedy and $b$ $\\varepsilon$-greedy) algorithm to control the inverted pendulum. The algorithm should be formulated as a function, such that we can pass $n$ and $\\sigma$ to it and get a policy and a reward history returned. For simplicity, we define $\\sigma=\\text{const.}$ during one training process.\n",
    "\n",
    "Carry out the training process for $n \\in \\{0,2,4,...,10\\}$ and for $\\sigma \\in \\{0,0.2,0.4,...,1\\}$. Evaluate the training process by determining the best reward history (how can we define what is a good reward history?).\n",
    "\n",
    "Parameters: $\\alpha=0.1, \\gamma=0.9, \\varepsilon=0.1$ with 500 time steps in 2 000 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0554811616de47a33023205031c51bd",
     "grade": false,
     "grade_id": "cell-079d114d4b940455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![](QSigma_1.png)\n",
    "![](QSigma_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f4a6fc99d80d9b5947db6735b039202",
     "grade": false,
     "grade_id": "cell-a291181972c262fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Solution 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15c3ffed48c941926c0ee45843b263a4",
     "grade": false,
     "grade_id": "cell-66f45e92e43daa8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This task will take some time both, in coding and in running the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac7e7bd9e731d1b3e8e4a370c55bda33",
     "grade": false,
     "grade_id": "cell-1f2bdfa05de72149",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def Q_sigma(n, sigma):\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    env = env.unwrapped\n",
    "    \n",
    "    action_values = np.zeros([d_theta, d_theta, d_omega, d_T])\n",
    "    pi = np.zeros([d_theta, d_theta, d_omega], dtype=int) # on policy: b=pi\n",
    "    \n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "    # sigma is an argument of this function\n",
    "    nb_steps = 500\n",
    "    nb_episodes = 2000\n",
    "    \n",
    "    reward_history = []\n",
    "    \n",
    "    for j in tqdm(range(nb_episodes), position=0, leave=True):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "991baefcb0834d6536c776507b458742",
     "grade": false,
     "grade_id": "cell-1679efea79ea461e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# install joblib with\n",
    "# >>> pip install joblib\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ns = np.arange(6) * 2\n",
    "sigmas = np.linspace(0, 1, 6)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3fc433db53ee659c8c27be0dc378c5d",
     "grade": true,
     "grade_id": "cell-a4fdeab624874b06",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ad459889d5454c6d11a66dfde991ec3",
     "grade": true,
     "grade_id": "cell-b8963cbbf439b147",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4637d81050202b1d099b025b65b08692",
     "grade": true,
     "grade_id": "cell-016c9036ab51035e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f70448fca9c6ba768923fabfd81a7639",
     "grade": false,
     "grade_id": "cell-17662c6ed4aabfd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "nb_steps = 500\n",
    "\n",
    "state = env.reset()  # initialize x_0\n",
    "disc_state = tuple(discretize_state(state))  # use tuple indexing\n",
    "disc_action = pi[disc_state]\n",
    "\n",
    "for k in range(nb_steps):\n",
    "\n",
    "    cont_action = continualize_action(disc_action)\n",
    "    env.render()  # comment out for faster execution\n",
    "    state, reward, terminated, _, _ = env.step(cont_action)\n",
    "    disc_state = tuple(discretize_state(state))\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "    disc_action = pi[disc_state]  # exploitative action\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
